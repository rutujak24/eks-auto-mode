Migrating Stateful Components

Overview | Migrate the catalog component's MySQL database
Overview

In our application, we have multiple stateful components:

    Orders PostgreSQL database
    Catalog MySQL database
    Checkout Redis in-memory
    Carts in-cluster DynamoDB

Sample Retail Application
Applications deployed in Amazon EKS can use several AWS storage services, including Amazon S3 
, Amazon EBS 
, and Amazon EFS 
, through an appropriate CSI driver 

.

For simplicity and brevity during this workshop, most of the stateful sub-components (Redis, PostgreSQL, and DynamoDB) use their pods' file system (via the emptyDir ephemeral volumes) or memory. Only the catalog component's MySQL database relies on the Amazon EBS CSI driver 

to provision an EBS volume to host its filesystem.

Specifically, it uses dynamic provisioning 

to trigger the automatic provisioning and attachment of an EBS volume when the MySQL StatefulSet is created.
Workshop stateful components

➤ We can specifically explore these components by executing the following command:

kubectl get sts -n apps -o wide

➤ We can verify that executing the following command and verifying that only a single pod, the MySQL StatefulSet, still runs on non-EKS Auto Mode compute:

kubectl get pods -n apps -o wide

Catalog component stateful components

➤ View the catalog component's MySQL StatefulSet pods:

kubectl get sts -n apps -l app.kubernetes.io/component=mysql

➤ View the underlying PersistentVolume and PersistentVolumeClaim:

kubectl get -n apps pv,pvc

➤ View the StorageClasses:

kubectl get sc

You can see the connection between these components:

Dynamic provision
Migration challenges

The main challenges in handling the migration of EBS-based stateful components that rely on dynamic provisioning can be summarized as follows:

    It is not possible to transfer the “ownership” of a PersistentVolume, as the spec.persistentvolumesource (which contains the reference to the handling driver) is immutable after creation.

    The StorageClass attribute in a PersistentVolumeClaim object is immutable and cannot be updated.

    It is not possible to attach both EBS CSI and Auto Mode (empty) volumes to the same node and mount them into the same Pod to perform an application-level migration of data.

Migrate the catalog component's MySQL database

We will demonstrate the migration of these components using the catalog component's MySQL as an example, but any stateful application that uses the EBS CSI driver in the same manner as above can follow the same process we are going to outline and execute below.

Additionally, for simplicity, we will call OSS EBS CSI driver-managed (and adjacent) resources “EBS CSI resources” and the Amazon EKS Auto Mode managed (and adjacent) resources “Auto Mode resources”.

In light of the challenges listed above, the migration will introduce a short downtime to the application – in this case, the catalog component's MySQL database.

For that purpose, we have developed a migration tool 

that automates the process to reduce operational overhead and minimize the incurred downtime.
ATTENTION

Note that the migration process requires deleting the existing PersistentVolumeClaim/PersistentVolume and re-creating them with the new StorageClass. You must validate this process in an identical non-production environment first.
Prerequisites

The migration process validates and requires that:

    The EBS volume behind the PersistentVolume must be unattached from any EC2 instances. This means scaling down the StatefulSet or Deployment to allow the volume to detach.

    The new StorageClass (that belongs to EKS Auto Mode) must have a volumeBindingMode of WaitForFirstConsumer to prevent the immediate creation of a PersistentVolume.

    The existing PersistentVolume must have a reclaim policy of Retain to ensure that the EBS volume remains when the PersistentVolume is deleted.

    The calling process of the tool needs the appropriate Kubernetes permissions to Create/Delete the PersistentVolumeClaim and PersistentVolume (see here 

    for more information).

    The calling process of the tool needs the appropriate AWS IAM permissions to call DescribeVolume, CreateTags on the EBS volume, and optionally, but recommended, CreateSnapshot.

➤ Execute the following:

export MYSQL_STS_NAME=retail-store-app-catalog-mysql
export MYSQL_POD_NAME=${MYSQL_STS_NAME}-0
export ORIGINAL_PVC_NAME=data-${MYSQL_POD_NAME}
echo ${ORIGINAL_PVC_NAME}

Download and install the tool

➤ Download the tool:

curl -sSL -o eks-auto-mode-ebs-migration-tool https://github.com/awslabs/eks-auto-mode-ebs-migration-tool/releases/download/v0.3.1/eks-auto-mode-ebs-migration-tool_Linux_x86_64

chmod +x eks-auto-mode-ebs-migration-tool

./eks-auto-mode-ebs-migration-tool

➤ Create a new EKS Auto Mode StorageClass manifest:

cat << EOF > am-storage-class.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp3-auto
provisioner: ebs.csi.eks.amazonaws.com
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  fsType: ext4
EOF

➤ Create the storage class:

kubectl apply -f am-storage-class.yml

➤ Execute the tool in the (default) dry-run mode to assess the changes planned to be done to the relevant objects and resources:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto

As you can see, the EBS volume is still attached to the EC2 instance on which the catalog component's StatefulSet Pod is running, so it can't yet be migrated, as mentioned in the prerequisites.

E0606 09:50:35.341373 296117 main.go:135] "Precondition checks failed" err="can't migrate volume vol-07698e9d8d3917a0b that is still attached to an instance"

➤ For the sake of completeness, verify that it is the same EC2 instance by extracting its name from the Pod and via the volume:

export ORIGINAL_PV_NAME=$(kubectl get -n apps pvc ${ORIGINAL_PVC_NAME} -o jsonpath='{.spec.volumeName}')
export VOLUME_ID=$(kubectl get -n apps pv ${ORIGINAL_PV_NAME} -o jsonpath='{.spec.csi.volumeHandle}')

# Get the node name where the MySQL Pod runs on
kubectl get pods -n apps ${MYSQL_POD_NAME} -o json | jq -r '.spec.nodeName'

# Get the node name that the EBS Volume of the PVC is attached to
aws ec2 describe-instances --instance-ids $(aws ec2 describe-volumes --volume-ids ${VOLUME_ID} --output text --query='Volumes[].Attachments[].InstanceId') \
  --output text \
  --query='Reservations[].Instances[].PrivateDnsName'

The expected output is the same node name twice (one from the kubectl command, and one from the aws cli command):

ip-192-168-104-189.us-west-2.compute.internal
ip-192-168-104-189.us-west-2.compute.internal

Before scaling down the catalog component's StatefulSet to allow the volume to detach, consider other factors that may interfere with the number of replicas, most notably active application autoscaling tools like Horizontal Pod Autoscaling (HPA) 
or KEDA 

.

It is recommended that you adjust their configuration (or disable them, if possible) to prevent scaling events during the EBS volume migration.

Our catalog component's StatefulSet doesn't have an HPA attached, so we can safely proceed with scaling it down.

➤ Execute the following command:

kubectl scale statefulsets -n apps ${MYSQL_STS_NAME} --replicas=0

During this time, the application should not be available.

➤ Execute the tool (after 10 - 15 seconds) again to verify that all prerequisites are fulfilled:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto

➤ View the current PV and PVC state:

kubectl get pv,pvc -n apps

This should produce a result similar to the following:

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                        STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
persistentvolume/pvc-c4bb4d38-3e92-4826-82d0-680c0d327bf7   10Gi       RWO            Retain           Bound    apps/data-retail-store-app-catalog-mysql-0   gp3            <unset>                          64m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-retail-store-app-catalog-mysql-0   Bound    pvc-c4bb4d38-3e92-4826-82d0-680c0d327bf7   10Gi       RWO            gp3            <unset>                 64m

➤ Execute the tool in the "mutate" mode (--dry-run=false) and answer YES (note the capitalization) when prompted:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto \
  --dry-run=false

➤ Once the migration is completed, view the current PV and PVC state:

kubectl get pv,pvc -n apps

The expected output should be similar to the previous one (both PV and PVC should have the Bound status), but with new IDs and a new, EKS Auto Mode CSI driver's storage class:

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                        STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
persistentvolume/pvc-6ac68662-ac3b-43f0-9fd3-c73b59c2e899   10Gi       RWO            Retain           Bound    apps/data-retail-store-app-catalog-mysql-0   gp3-auto       <unset>                          23m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-retail-store-app-catalog-mysql-0   Bound    pvc-6ac68662-ac3b-43f0-9fd3-c73b59c2e899   10Gi       RWO            gp3-auto       <unset>                 23m

Note that we have to execute the migration tool for each of the relevant PVCs, for example for data-retail-store-app-catalog-mysql-1, data-retail-store-app-catalog-mysql-2 etc...

After migrating all the PV and PVC objects to use the Auto Mode storage capability, we need to update our application (the catalog StatefulSet) to point to the new StorageClass that uses Auto Mode capability for storage too. Since the StatefulSet's storageClassName field is immutable, we will first need to delete the StatefulSet, and then reapply it with the right StorageClass configuration (and amount of replicas).

➤ Delete the StatefulSet of the catalog-mysql:

1
kubectl -n apps delete sts retail-store-app-catalog-mysql

We now will scale the application by updating the catalog component's Helm values.yml file, which would both migrate the MySQL Pod to use EKS Auto Mode compute and reset the number of replicas to 1.

For your applications, you should apply the above steps in a way that fits your configuration.

➤ Update the catalog-values.yml file for the catalog component (see here 

for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
cat << EOF > catalog-values.yml
replicaCount: 3

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule

app:
  persistence:
    provider: mysql
    endpoint: ""
    database: "catalog"

    secret:
      create: true
      name: catalog-db
      username: catalog
      password: "mysqlcatalog123"

mysql:
  nodeSelector:
    role: apps-auto-mode
  tolerations:
    - key: role
      value: apps-auto-mode
      operator: Equal
      effect: NoSchedule
  persistentVolume:
    storageClass: "gp3-auto"
EOF

Pay attention to the new storageClass configuration of gp3-auto.

➤ Update the catalog component:

1
2
3
4
5
6
helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values catalog-values.yml \
  --reuse-values \
  --wait

➤ After a few moments, verify that we can access the application UI, via the ALB by executing the following command to extract the DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main-auto-mode \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ Verify that all pods are scheduled on the apps-auto-mode nodes by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|")')
kubectl get pods -n apps -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

➤ View the distribution of instances across capacity types:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The result should be similar to the following:

i-085c7fe5654a651c8                            |  us-west-2a  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0ce7c07eb76652c22                            |  us-west-2c  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0d26a453b47d25d9f                            |  us-west-2b  |  KARPENTER  |  nodepool:   apps-auto-mode
...
ip-192-168-112-81.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-150-83.us-west-2.compute.internal   |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-184-40.us-west-2.compute.internal   |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-133-200.us-west-2.compute.internal  |  us-west-2b  |  ON_DEMAND  |  system-mng  
ip-192-168-171-235.us-west-2.compute.internal  |  us-west-2c  |  ON_DEMAND  |  system-mng  
ip-192-168-99-105.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  system-mng  

We have now migrated our retail store application to EKS Auto Mode and all that remains is to remove the no-longer-required add-ons and compute resources that belong to the now-unused managed node groups!
